<!DOCTYPE html>
<head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-108611659-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-108611659-1');
	</script>
	<title>Buyun Wang</title>
	<meta charset="UTF-8">
	<meta name="author" content="Buyun Wang">
	<meta name="description" content="Buyun(Brian)'s Personal Website">
	<meta name="keywords" content="front-end, developer, engineer, Buyun, Wang, Brian, web, developer, engineer, HTML, CSS, JavaScript, jQuery, Java, Python, Nodejs, MongoDB, React, LoopBack">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.css" media="screen">
	<!-- Main CSS-->
	<link rel="stylesheet" type="text/css" href="style.css">
	<!--Animation -->
	<link href="css/hover.css" rel="stylesheet"/>
	<link href="css/animate.css" rel="stylesheet"/>
	<link href="css/waypoints.css" rel="stylesheet"/>
	<!--jQuery Smooth Scroll-->
    <link rel="stylesheet" href="http://code.jquery.com/ui/1.11.4/themes/smoothness/jquery-ui.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <!--Core JS files -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
    <script async="" src="https://www.google-analytics.com/analytics.js"></script>
  	<script src="http://code.jquery.com/jquery-1.10.2.js"></script>
  	<script src="http://code.jquery.com/ui/1.11.4/jquery-ui.js"></script>
	<script src="js/jquery.waypoints.min.js" type="text/javascript"></script>
	<script src="js/waypoints.js" type="text/javascript"></script>
	<script>
	$(document).ready(function(){
	  // Add smooth scrolling to all links
	  $("a").on('click', function(event) {

	    // Make sure this.hash has a value before overriding default behavior
	    if (this.hash !== "") {
	      // Prevent default anchor click behavior
	      event.preventDefault();

	      // Store hash
	      var hash = this.hash;

	      // Using jQuery's animate() method to add smooth page scroll
	      // The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
	      $('html, body').animate({
	        scrollTop: $(hash).offset().top
	      }, 800, function(){
	   
	        // Add hash (#) to URL when done scrolling (default click behavior)
	        window.location.hash = hash;
	      });
	    } // End if
	  });
	    // Toggle tranparent navbar when the user scrolls the page

	  $(window).scroll(function() {
	    if($(this).scrollTop() > 50)  /*height in pixels when the navbar becomes non opaque*/ 
	    {
	        $('.opaque-navbar').addClass('opaque');
	    } else {
	        $('.opaque-navbar').removeClass('opaque');
	    }
	});
	});


</script>
</head>

<body>

<nav class="navbar navbar-inverse navbar-fixed-top opaque-navbar">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navMain">
  <span class="glyphicon glyphicon-chevron-right" style="color:white;"></span>
    
  </button>
      <a class="navbar-brand" href="#page-top">BRIAN WANG</a>
    </div>
    <div class="collapse navbar-collapse" id="navMain">
      <ul class="nav navbar-nav pull-right navbar-right">
      	<li class="hidden page-scroll"><a href="#page-top"></a></li>
        <li class="page-scroll nav-btn"><a href="#about">About</a></li>
        <li class="page-scroll nav-btn"><a href="#projects">Projects</a></li>
        <li class="page-scroll nav-btn"><a href="#contact">Contact</a></li> 
     
      </ul>
    </div>
  </div>
</nav>

	<section class="intro" id="page-top">
		<div class="inner">
			<div class="content">
				<section class="os-animation" data-os-animation="fadeIn" data-os-animation-delay="1s">
					<h1>Brian Wang</h1>
				</section>
				<section class="os-animation" data-os-animation="fadeIn" data-os-animation-delay="1.5s">
					<h2>Web / Software Developer</h2>
				</section>
				<section class="os-animation" data-os-animation="fadeIn" data-os-animation-delay="2.0s">
					<a class="btn page-scroll" href="#about">Get to know me</a>
				</section>
			</div>
		</div>
	</section>
	
	<div class="container-fluid" id="about">
		<div class="row no-padding">
			<div class="col-md-6 col-sm-6 no-padding" >	
	        <img src="images/head-reduced.jpg" class="img-responsive os-animation" style="margin-left:0px;" data-os-animation="fadeIn" data-os-animation-delay=".5s"></img>
			</div>
			<div class="col-md-6 col-sm-6" id="abouttext">
                <h2 class="os-animation heading" data-os-animation="fadeIn" data-os-animation-delay=".5s" id="heading1">About Me</h2>
				<p class="os-animation" data-os-animation="fadeIn" data-os-animation-delay="1s">Hi, my name is Buyun Wang. I am a computer science new grad from UBC. After learning all kinds of algorithms, languages and programming skills, I am always eager to test my abilities in a practical way. I am a quick learner and tend to have a good grasp of fast evolving technologies. I am also a team player as I have been working in several companies and have experience in both mobile application development and website development. </p>
				<p class="os-animation" data-os-animation="fadeIn" data-os-animation-delay="1s">I am interested in the front end development as I am majorly a visual learner and I am quite intrigued by user interface and how users interact with the program. Computer graphics is another area that I have set my foot in with shading language like OpenGL and APIs like Three.js. The languages I am most familiar with inclue Python, C++, Java, JavaScript, HTML5 and CSS3. I also have experience with Node.js, Bootstrap and Object Oriented Programming. </p>
				<h2 class="os-animation heading" data-os-animation="fadeIn" data-os-animation-delay="1.5s" id="heading2">Need help with the design?</h2>
				<p class="os-animation" data-os-animation="fadeIn" data-os-animation-delay="2s">
				The design skills I acquired have enabled me to deliver an idea from prototype to final product. It includes Photoshop, GIMP and Illustrator. 				
				<br>   <br>
				<!-- <a class="hover hvr-icon-spin" href="https://www.behance.net/brianwang" style="color:black; padding:0 25px;" target="_blank"> /brianwang</a>  --> 
                <a class="hover hvr-grow" href="https://www.behance.net/brianwang" style="color:black;"><i class="fa fa-behance" aria-hidden="true"></i> /brianwang</a> </br>
				</p>
			</div>
		</div>

		<hr class="divider" id="projects">
        <h2 class="divider-heading"> Projects</h2>
        <hr class="divider" >
  <!--       <h2 class="heading">All Projects</h2> -->
<!-- 	
		<div class="row no-padding project">
			<div class="col-md-7">
		</div>
		</div> -->
	    <div class="row no-padding project">
        <div class="col-md-7 os-animation" data-os-animation="slideInLeft" data-os-animation-delay=".2s">
          <a href="#portfolioModal4" class="portfolio-box hover hvr-grow" data-toggle="modal" style="color:black;">
            <h2 class="project-heading">Shortcutters App </br><span class="text-muted"> UI&UX Design</span></h2>
            <p class="lead">A group project focusing on the research of advanced methods for human-computer interaction. </p>
	      </a>
        </div>
        <div class="col-md-5 container">
	    	<a href="#portfolioModal4" class="portfolio-box hvr-grow" data-toggle="modal">
	          <img class="portfolio-image-r img-responsive center-block" src="images/shortcutter_cover.jpg" alt="Generic placeholder image">
	        </a>  
        </div>
        </div>

        <hr class="divider">

		<div class="row no-padding project">
        <div class="col-md-7 col-md-push-5 os-animation" data-os-animation="slideInRight" data-os-animation-delay=".2s">
          <a href="#portfolioModal3" class="portfolio-box hover hvr-grow" data-toggle="modal" style="color:black;">
            <h2 class="project-heading">Computer Graphics Projects </br><span class="text-muted">Three.js, WebGL, Vertex Shaders</span></h2>
            <p class="lead">A series of graphics projects that utilizes the power of three.js Javascript 3D library. </p>
	      </a>
          
        </div>
        <div class="col-md-5 col-md-pull-7 container">
            <a href="#portfolioModal3" class="portfolio-box hvr-grow" data-toggle="modal">
              <img class="portfolio-image-l img-responsive center-block" src="images/graphics_cover.jpg" alt="Generic placeholder image">
            </a>  
        </div>
        </div>
 
        <hr class="divider">

  	    <div class="row no-padding project">
        <div class="col-md-7 os-animation" data-os-animation="slideInLeft" data-os-animation-delay=".2s">
          <a href="#portfolioModal2" class="portfolio-box hover hvr-grow" data-toggle="modal" style="color:black;">
            <h2 class="project-heading">DrinkSmart Web Application </br><span class="text-muted"> GWT, JavaScript, OO Design</span></h2>
            <p class="lead">A web applet using Google's app engine and BC government's database to help cyclists to find the drinking fountains along their routes. </p>
	      </a>
          
        </div>
        <div class="col-md-5 container">
        	<a href="#portfolioModal2" class="portfolio-box hvr-grow" data-toggle="modal">
	          <img class="portfolio-image-r img-responsive center-block" src="images/drinksmart_cover1.jpg" alt="Generic placeholder image">
	        </a>  
        </div>
        </div>

        <hr class="divider">

		<div class="row no-padding project">
        <div class="col-md-7 col-md-push-5 os-animation" data-os-animation="slideInRight" data-os-animation-delay=".2s">
          <a href="#portfolioModal1" class="portfolio-box hover hvr-grow" data-toggle="modal" style="color:black;">
            <h2 class="project-heading">DriveEarth - Collaborative Mobility </br><span class="text-muted">UI Design, Prototyping</span></h2>
            <p class="lead">A carpooling application designed to tackle the ongoing transportation issue by harnessing the power of sharing economy.</p>
	      </a>
          
        </div>
        <div class="col-md-5 col-md-pull-7 container">
        	<a href="#portfolioModal1" class="portfolio-box hvr-grow" data-toggle="modal">
	          <img class="portfolio-image-l img-responsive center-block" src="images/driveearth_cover.png" alt="Generic placeholder image">
	        </a>  
        </div>
        </div>

        <!-- Portfolio Modals -->

        <div class="portfolio-modal modal fade" id="portfolioModal1" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
               <i class="fa fa-times close-btn-s" aria-hidden="true"></i>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-10 col-lg-offset-1" style="text-align: left;">
                        <div class="modal-body">
                            <h2 class="title-modern" style="text-align: center;">DriveEarth Mockup & Wireframing Design </h2>
                            <hr class="star-primary">
                            
                            <p>During the hackathon and entreprenuership program held by <a class="hover" href="http://inacademy.eu/" target="_blank">European Innovation Academy</a> in the summer of 2014, our team worked on a project to connect people on the road --- a mobile application called DriveEarth(now <a class="hover" href="http://wever.fr/" target="_bland">Wever</a>). </p>
                            <embed id="slide" src="images/driveearth_slide.pdf" width="700px" height="550px" style="margin-left:100px;"/>
                            <!-- <img class="img-responsive" src="assets/img/portfolio/bbm/binder-sign.jpg"> -->
                            
                            <h3>My Role</h3>
                            <p>The final product is a workable and responsive prototype that was built with Fluid UI. It contains all of the user cases we were trying to address and combines a fun eco-friendly framework (gamification) with a real time and traditional carpooling service to drivers and passengers. I took on the role to design app wireframing, icons, logo, pitch slides and typography of drive-earth.net(currently <a class="hover" href="http://wever.fr/" target="_bland">wever.fr</a>), which led to the creation of an app mockup and the first version of our website.  </p>
                        
                            <p>To achieve the fun aspect of our carpooling application(gamification), a set of character designs(birdies) were carried out using Adobe Illustrator and Adobe Photoshop. Character attributes were taken from RPG games to give a sense of developing a perfect digital companion while ordering/sharing rides with others. The tuned down version of the neon color scheme was a tribute to games from the 80s and 90s.</p>
                        
                            <h3>Tools</h3>
                            <p>FluidUI, HTML, CSS, JavaScript, Bootstrap, JQuery, Adobe Photoshop, Adobe Illustrator</p>
                         
                            <button type="button" class="btn btn-default close-btn" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
	        </div>
	    </div>

        <div class="portfolio-modal modal fade" id="portfolioModal2" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
               <i class="fa fa-times close-btn-s" aria-hidden="true"></i>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-10 col-lg-offset-1" style="text-align: left;">
                        <div class="modal-body">
                            <h2 class="title-modern" style="text-align: center;">DrinkSmart with OO Design</h2>
                            <hr class="star-primary">
                            
                            <p>DrinkSmart is a CPSC310 course project and a group effort of 4, following a scrum software team process and an agile process model. Making use of the dataset of drinking fountains from the city of Vancouver, DrinkSmart aims to help joggers, runners and bikers to easily get hydrated without carrying around water bottles on the road. Users can view, save, rate and review drinking fountains by logging in to their Facebook accounts or registerring new accounts on the website. The comments are shared with all users registered or unregistered to save time while doing sports activity. </p>
                            
                            <!-- <iframe src="http://3.drinksmart001.appspot.com/" width="80%" height="700" frameborder="0" class="centered"></iframe> -->
                         
                            <h3>My Role</h3>
                            <p>The final product is a responsive web AJAX-rich appliaction built with GWT using an object oriented design pattern. I took on the role to design data object, build UI and manage events. To enrich the functionality and ease of use, the interface is kept clean and simple without rich colors to distract user's attention from the map. A black geo tag differenciates itself from the red ones which is designed to be moved by the user to display a list of drinking fountains near a specific location. Users can view the top 10 rated drinking fountains and their personal collection easily by tapping on the different tabs of the table list which is sorted alphabetically to switch view. A fullscreen view can be found <a class="hover" href="http://3.drinksmart001.appspot.com/">here</a>.</p>
                       
                            <h3>Tools</h3>
                            <p>Java, CSS, GWT, AJAX, JUnit, Adobe Photoshop</p>
                         
                            <button type="button" class="btn btn-default close-btn" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
	        </div>
	    </div>

        <div class="portfolio-modal modal fade" id="portfolioModal3" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
               <i class="fa fa-times close-btn-s" aria-hidden="true" ></i>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-10 col-lg-offset-1" style="text-align: left;">
                        <div class="modal-body">
                            <h2 class="title-modern" style="text-align: center;">Graphics Programming with Three.js and WebGL</h2>
                            <hr class="star-primary">
                            
                            <p>The projects are completed during CPSC 314 in UBC. Template codes are provided by <a class="hover" href="https://www.ugrad.cs.ubc.ca/~cs314/Vsep2015/">Mikhail Bessmeltsev</a>. This course provides an introduction into the basic components of computer graphics including the mathematics and physics for computer graphics, foundations of 3D rendering, image processing and rendering pipeline.  </p>
                            
                            <h3>Animated Armadillo</h3>
                            <img class="img-responsive" src="images/armadillo.jpg" style="padding: 0 10%;">
                            <p>The template provides a simple scene with a yellowish 3D display, working normally and displaying an armadillo, and a non-functioning red remote control. The variable remotePosition is changed using arrow keys and passed to the remote control shader (remote.vs.glsl) using uniform variables in response to keyboard input. The remote control’s fragment shader (remote.fs.glsl) is modified so that it changes its color according to a uniform variable tvChannel. You are able to switch channels using your keyboard (keys 1-3) and see the remote react. The fragments that are within some distance of the remote changes color due to the modification to the character’s shaders. The armadillo is distorted in different ways for different channels including deforming the vertices in the armadillo in a wave over time and exploding the model along face normals. </br><a class="hover" href="graphics/a1/A1.html" target="_bland">See it in action here.</a>  </p>  
	                        </br>
	                        </br>
                            
                            <h3>Ostrich with Transformation</h3>
                            <img class="img-responsive" src="images/ostrich.jpg" style="padding: 0 10%;">
                            <p>The template provides a torso, (placed with respect to the world coordinate frame) a neck, (placed with respect to the torso coordinate frame) and a head (placed with respect to the neck coordinate frame) of the ostrich leaving the legs to be completed. The legs I added consist two parts, both of which are cylinders: a thigh, (placed with respect to the torso coordinate frame) and a lower leg (placed with respect to the thigh coordinate frame). The thigh and lower leg coordinate frames are rotated so that they move like the ostrich is running. The lower leg not only moves with respect to the thigh, but also has an independent motion. The running animation is cyclical and continues to move back and forth. They are moving at a reasonable speed. A timer is used to keep animation speed consistent instead of animating the legs every frame. Five different poses for the static body coordinate frames are added. Pressing the number keys between 0 and 5 should each show a different pose.</br><a class="hover" href="graphics/a2/A2.html" target="_bland">See it in action here.</a>  </p>  
                            </br>
                            </br>

                            <h3>Shading, lighting and Textures</h3>
                            <img class="img-responsive" src="images/shading.jpg" style="padding: 0 10%;">
                            <p>This project demonstrates couple of common shading methods including Gouraud Shading, Phong Reflection and Phong Shading, Blinn-Phong Shading and some basic texturing. The lightColor, lightDirection and ambientColor variables are defined as well as material properties for the objects(defined in kSpecular, kDiffuse and kAmbient).
                        	Gouraud shading calculates the lighting of an object at each vertex (e.g. in the vertex shader). The standard graphics pipeline will then interpolate the color of the object between the vertices to get the individual fragment colors. The Phong reflection model is achieved by passing the lighting parameters (such as the triangle normal and vertex position) as varying variables to the fragment shader. Then calculate the lighting of the mesh at each fragment using the interpolated lioghting parameters. This allows the shading model to better approximate curved surfaces and produces a very smooth shading model. The main calculations are done in the fragment shader. Blinn-Phong Shading is achieved by computing the dot product between the halfway vector between light and viewing direction, and the surface normal. In the texturing part, UV coordinates are taken from the vertex buffer (three.js provides them for free on default objects such as the sphere in our assignment). UV coordinates are a 2D index into an image file whoose RGB values can be used to help color the mesh.</br><a class="hover" href="graphics/a3/A3.html" target="_bland">See it in action here.</a>  </p> 
                        	</br>
                        	</br>        

                        	<h3>Ray Tracing</h3>
	                        </br>
                            <img class="img-responsive" src="images/raytracer.bmp" style="padding: 0 10%;">
                            <p>The first step to implement ray tracing is the basic ray casting for all pixels in the image, using the camera location and the coordinates of each pixel. Then iteratively test all the object’s intersection with the given ray and update the depth as the first intersection’s depth. The functions of intersection tests and the functions of casting primary rays are implemented in regard to different geometries to test if a ray intersects the objects. Next step is local illumination. It's achieved by calculating the ambient, diffuse, and specular terms in order to determine the color at the point where the ray intersects the scene. Then to implement the shadow ray calculation and update the lighting computation accordingly, shadow ray is emitted from a point to compute direct illumination to determine which lights are contributing to the lighting at that point. Finally to implement the secondary ray recursion for reflection, the rayDepth recursion depth variable is used to stop the recursion process and update the lighting computation at each step to account for the secondary component.</p> 
                        	</br>                 
                        
                            <h3>Tools</h3>
                            <p>JavaScript, GLSL, Three.js, C++, Some math skills</p>
                         
                            <button type="button" class="btn btn-default close-btn" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
	        </div>
	    </div>


        <div class="portfolio-modal modal fade" id="portfolioModal4" role="dialog" aria-hidden="true">
        <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
               <i class="fa fa-times close-btn-s" aria-hidden="true"></i>
            </div>
            <div class="container">
                <div class="row">
                    <div class="col-lg-10 col-lg-offset-1" style="text-align: left;">
                        <div class="modal-body">
                            <h2 class="title-modern" style="text-align: center;">Shortcutters App with UX Design</h2>
                            <hr class="star-primary">
                            
                            <p>Shortcutters app is a CPSC444 course project and a group effort of 5, focusing on the advanced methodologies of human-computing interactions. This page shows the experiment report of this project along with the prototype. The main purpose of this study is UX research, the methods used to conduct user testing, and how to analyse the data produced to write a research report.
                            <a class="hover" href="https://cs444theshortcutters.wordpress.com/">The detailed journal of the progress can be found in our blog.</a></p>
                            </br>
                            <div class="embed-responsive embed-responsive-4by3">
                                <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/RpNCj4dxwbE?rel=0" style="padding: 0 10%;"></iframe>                               
                            </div>
                            <p style="text-align:center;"><i>A video introduction</i></p>

                            <h3>My Role</h3>
                            <p>To create responsive low to high fidelity prototype for a range of tests from cognitive walkthrough to experiment design, we opted for Justinmind to satisfy our prototyping and wire framing needs. I was responsible for the creation of various user interface design, iterations of workflow design, storyboarding and experiment design with data analysis tools.</p>

                            <h3>Tools</h3>
                            <p>Justinmind, Adobe Illustrator, Teamwork</p>

                            <hr class="divider"> 

                            <h3>1. Introduction</h3>
                            <p>Getting around in a city in style is becoming more and more popular around young generations. We see various kinds of applications providing this type of functionalities come on the stage from time to time, ranging from carpooling applications that allow different people to easily share ride with each other depending on the need, to smart navigation apps that crowdsources real-time data from drivers to personalize best possible routes. Our prototype is about a crowdsourcing navigation app that offers personalized routes choosing experience in a social setting. Current popular navigation apps like Google Maps and Waze seem to miss out on this fantastic feature. </br></br> This report describes the experiment instrument and method in details, discusses the problems and limitations encountered and the relation to other works for future work and practitioners. </p>

                            <h3>2. Description of the Experiment</h3>
                            <h4>2.1 Introduction and goals </h4>
                            <p>Our prototype focused on discovering alternative routes to a destination. These routes could be in the city, on a hike. They could be on the map, or more than likely the novelty of our solution will be that we want to help users discover routes that may not be on maps. After conducting field study and cognitive walkthrough in milestone II, we learned that potential users may choose their routes on focal points like distance/time, safety and scenery. One of the biggest challenge would be to decide what types of look and feel is the best for our user to quickly and accurately select the route that caters to their needs the best. </br>Our main goal of the experiment therefore is to determine what filter results should be displayed to maximize user confidence in a route. To further expand our goal we are testing our interface based on two different variations, how the filter percentage results are displayed, and how the best matching filter is displayed. We also had two subgoals:</br> 1. To determine if verbal or visual representations in “best filter match” leads to a higher confidence.</br>2. To determine with factor level leads to the quickest user decision.</br> </br> We believed that percentage report for each specific focal point user can alter in filter together with text should work the best theoretically. </p>


                            <h4>2.2 Methods </h4>
                            <h5>2.2.1 Participants</h5>
                            <p>We recruited 10 participants (2-3 participant per group member) not including pilot test. The participants consisted of mainly students over 19 years old, either randomly sampled on UBC campus or recruited through friends.  They were given little instructions to begin and didn’t fill out any questionnaire.  These were representative users even they didn’t fully cover the target demographic types. </p>

                            <h5>2.2.1 Participants</h5>
                            <p>We recruited 10 participants (2-3 participant per group member) not including pilot test. The participants consisted of mainly students over 19 years old, either randomly sampled on UBC campus or recruited through friends.  They were given little instructions to begin and didn’t fill out any questionnaire.  These were representative users even they didn’t fully cover the target demographic types. </p>

                            <h5>2.2.2 Conditions</h5>
                            <p>We are comparing which of the following methods/different interface types of showing filter results gives users maximum confidence in a route:</br></br>
                            	<div class="list">
                            	<ol>
                            		<li>Short verbal descriptions (eg. “Perfect match!” or “you can live with it…”)
                            			<ul>
                            				<li>Plus a little icon for the category it’s best matched to (eg. rocket or scenic photo, etc.)</li>
                            				<li>Or a verbal description of the category it’s best matched to (e.g. “scenic”, “Fast”)</li>
                            			</ul>
                            		</li>
                            		<li>Percent match (as a number) for each of the categories (rain cover, speed, scenery, etc.)
                            			<ul>
                            				<li>Plus a little icon (same as above)</li>
                            				<li>Or a verbal description (same as above)</li>
                            			</ul>
                        			</li>
                        			<li>Aggregated/ combined percent match, as in one big number encompassing a weighted average of the percent match in all categories.
                        				<ul>
                        					<li>Plus a little icon (same as above)</li>
                            				<li>Or a verbal description (same as above)</li>
                            			</ul>
                            		</li>	
                            	</ol>
	                            </div>
	                        </p>

	                        <h5>2.2.3 Tasks</h5> 
	                        <p>Participants were asked to perform three tasks. All tasks aim to test the main feature of our application which is to quickly find a route based on a set of desired preferences. In the first task, participants were given a scenario where they value scenery the most; this means the first slider on the filter page should be adjusted to be closest to the scenic end, and the participants can freely adjust the other two scrollbars. After doing so, the participants were asked to select a route from the result page that best matches the criteria. Similar as the first task, in the second task, the task is targeted to the level of safeness of a route. In the third task, the third scrollbar will be targeted, and participants were asked to find a route which is mainly set outdoors.</br></br>Task 1:  Imagine you a UBC student trying to get from your residence in Totem Park to your class in Buchanan A. You are not tight on time so you prefer to take a route that is scenic. Your class is in the middle of the day and it is bright and sunny outside so you don’t have much of a preference about safeness or rain cover. </br></br>Task 2:  Imagine you are commuting from work place in downtown Vancouver and you want to get to your home in East Hastings. It is 6:30pm and dark outside, it is not raining. You prefer to take a route that is clear of any back-alleys and is properly lit at all times. You are not in a big hurry to get home.</br></br> Task 3:  Imagine you are spending the day with your family on Granville Island. You wish to get from the parking lot to the docks and prefer to avoid the crowds indoors. You are not in a hurry and there is no sense of danger around.</p>

	                        <h5>2.2.4 Design</h5>
	                        <p>This experiment was conducted using a 1x6 within subject design, with 1 level of expertise and 6 interfaces that were tested. Performance(time) and satisfaction(confidence level) were both measured within subjects. Each participant completed one task on each of the six conditions, for a total of 6 trials. For each trials we looked at three metrics for evaluation: time, error tally and confidence level. We used timer to record time lapses, number of corrections for error tally and participants rating on a scale from 0 - 10 for confidence level. We used 6 tasks in total for the experiment. Although the tasks were designed to have similar difficulties but depending on the background of participants one scenario may appear to be harder to imagine for one participant than others. Therefore we gave user random task and condition pair for 6 trials to minimize the potential negative learning curve effect on every first trial. </p>

	                        <h5>2.2.5 Procedure</h5>
	                        <div class="list">
	                        	<ol>
                            		<li>Before the Participant Begins:
                            			<ul>
                            				<li>Start a timer app on your mobile device, and ensure that it has a “lap” feature.</li>
                            				<li>Randomly select an order to conduct the trials in. I’d recommend using https://www.random.org/lists/ to do this.</li>
                            				<li>Open the medium fidelity prototype, and navigate to the search display for the first randomly selected trial.</li>
                            				<li>Ensure that you have a notepad and a pen. Begin by recording the randomized ordering of trials that you generated.</li>
                            			</ul>
                            		</li>
                            		<li>When the participant arrives:
                            			<ul>
                            				<li>Request that they sign a consent form.</li>
                            				<li>If the participant is willing to share, record their age and gender identity.</li>
                            				<li>Show them a few of the primary screens on the medium fidelity prototype, and offer a very brief (2-3 sentence) explanation of our design, and what it seeks to accomplish.</li>
                            				<li>Ask if the participant has any last questions before you begin.</li>
                            			</ul>
                        			</li>
                        			<li>Running a trial:
                        				<ul>
                        					<li>For each of the 6 conditions and 3 tasks, record which condition and which task you’re conducting the experiment in.</li>
                            				<li>Give the participant the corresponding task (see above) and tell the participant from where to where they will be selecting a route.</li>
                            				<li>Ask them if they’re familiar with the area, and if so, record their familiarity as a rating from 1 (never seen before) to 5 (walk there on a daily basis).</li>
                            				<li>Start the timer.</li>
                            				<li>Ask the participant to select a route from those provided by the search results that best matches the scenario in the task.</li>
                            				<li>Any time the user seems confused or makes clicks on something that might lead them away from the search results, please:
                            					<ul>
                            						<li>record the time at which this happens with the “lap” button</li>
                            						<li>make a short (1 sentence) note about the confusion/error</li>
                            						<li>and provide assistance as necessary.</li>
                            					</ul>
                            				</li>
                            				<li>When the user finally chooses the route they would like to take, stop the timer and record that time.</li>
                            				<li>Ask the user how confident they feel that this will be the best route for them expressed on a scale of 1 (they wouldn’t use this route) to 5 (this is the best possible path to where they’d like to go).</li>
                            				<li>If they do anything unexpected or interesting, make note of it.</li>
                            				<li>Ask them “How trustworthy do you think these results are?” and “How did you select the route you did”.</li>
                            				<li>Finally, review the recorded “laps” and attribute these times to your notes of hours.</li>
                            				<li>Reset the timer, and return to the beginning of section 3, Running a Trial.</li>
                            			</ul>
                            		</li>
                            		<li>Wrap-Up:
                            			<ul>
                            				<li>At the end of all the trials, take a photo of your notes as a backup.</li>
                            				<li>Stash the consent form somewhere safe.</li>
                            				<li>Thanks the participant for their willingness to help out!</li>
                            			</ul>
                            		</li>	
                            	</ol>
	                        </div>

	                        <h5>2.2.6 Apparatus </h5>
	                        <p>In ICICS X360, we gave participant a laptop with prototype opened up in browser and let participant perform task. One team member observe the user, while another team member takes notes and record time with a stopwatch.	</p>

	                        <h5>2.2.7 Independent and dependent variables</h5>
	                        <div class="list">
	                        	<ul>
	                        		<li>Independent: Methods of showing filter results (short verbal, percent match, aggregated percent match, plus verbal or icon for best match).</li>
	                        		<li>Dependent: Time length, Error tally, User confidence level (on scale 1-10).	</li>
	                        	</ul>
	                        </div>

	                        <h5>2.2.8 Hypotheses</h5>
	                        <div class="list">
	                        	<ul>
	                        		<li>Aggregated percent match with text descriptions will give users maximum confidence in result selected. (That is: H0: Conftext agg.  <=  Confnumerical agg. = Conficon agg.) </li>
	                        		<li>Text will perform better over icons for best filter match as some users might have different interpretations on what icons are meant to convey on the search results. (That is: H0: ttext  >=  tnumerical = ticon)	</li>
	                        	</ul>
	                        </div>

	                        <h4>2.3 Problems/Limitations</h4> 
	                        <p>One of the problems we encountered was that user was not be able to tell the difference of confidence level perceived between the 6 different methods of display. Some participants were not familiar with some task scenarios. Also, how well participants are familiar with variety of apps and/or social media sites had an effect on how well the participant performs (e.g. if a user is familiar with Stack Overflow or Reddit, they will be well familiar with the upvote/downvote arrows, while participant that rarely uses these social media might get confused with what they are).</p>
                            
                            <h3>3. Results </h3>
                            <p>In our test we used one-way ANOVA with alpha=0.05 to test for difference between interfaces in terms of confidence level and time. </br></br>Table of differences in confidence across different factors:</br>
                            <table style="width:80%; margin:0% 10%;">
							  <tr>
							  	<th></th>
							    <th>Icon vs Verbal</th>
							    <th>Tasks</th> 
							    <th>Disp:Age</th>
							  </tr>
							  <tr>
							  	<td>Sum</td>
							    <td>1.19</td>
							    <td>0.03</td>
							    <td>15.15</td>
							  </tr>
							  <tr>
							  	<td>Mean</td>
							    <td>1.19</td>
							    <td>0.03</td>
							    <td>7.574</td>
							  </tr>
							  <tr>
							  	<td>F-value</td>
							    <td>0.245</td>
							    <td>0.008</td>
							    <td>2.337</td>
							  </tr>
  							  <tr>
							  	<td>Pr(>F)</td>
							    <td>0.623</td>
							    <td>0.928</td>
							    <td>0.108</td>
							  </tr>
							</table>
							</br>
							<p>From the charts below we can see aggregated percent match with text descriptions gives users most confidence in completing the tasks while aggregated percent match with icon costs users least time. </p>
							</br>
							<img class="img-responsive" src="images/mean-time-per-trial.png" style="padding: 0 10%;">
							</br>
							<img class="img-responsive" src="images/mean-conf-per-trial.png" style="padding: 0 10%;">
							</br>
							<img class="img-responsive" src="images/time-per-trial.png" style="padding: 0 10%;">
							</br>
							<img class="img-responsive" src="images/conf-by-time.png" style="padding: 0 10%;">
                            </p>

                            <h3>4. Discussion </h3>
                            <h4>4.1 Interpretation of results</h4>
					        <p>Unfortunately due to our small sample size, there is no significant differences in confidence across our different factors where we hope to see some positive result. The most significant thing we get to see is the difference is in the interaction effects between task and trial, but this is unreliable given the miniscule sizes of these subdivided groups. Therefore we failed to reject any of our null hypothesis.</p>
					        <h4>4.2 Relation to other works</h4>
					        <p>Our study showed small improvements in relation to other experiment in as sense that we have used a very scientific approach but this field of display testing is old.</p>
					        <h4>4.3 Impact for practitioners</h4>
					        <p>After conducting the experiment we believe that cell phone may be the best media to test a mobile application design because of the familiarity brought by the phone. </p>
					        <h4>4.4 Critical reflection </h4>
					        <p>After our initial piloting phase and cognitive walkthrough (see Blog Update #3), we learnt that we needed to develop a better aspect of our interface to test. From our original low-fidelity paper prototype, we decided to design five other ways in which we could present our results to users. This was a big change in our interface’s design as it required a full analysis of all of our interface’s elements in order to determine how, and in which combinations, they should be presented to our users.</br></br>It was surprising to learn that users preferred a full breakdown of the filter statistics (individual percent matches) as compared to combined or verbal percentages. In theory, we thought that people would prefer the combined percentages as it provides less information to process than a multiple of percentages which takes more time to process. It was also interesting to observe that participants performed the tasks faster when given an icon instead of a text label for the best filter match. </br></br>The methods that we choose to use for our evaluation allowed us to collect the basics of what we were looking for. As mentioned in Blog Update #7a, it would have been best to have tested each condition using multiple tasks, however, respect for our participant’s time and our own time restrictions prevented us from performing a more in-depth study. It would have also have been helpful if we had a post-experiment questionnaire ready to collect a consistent set of observations rather than freely asking questions after each trial. Our chosen method may have caused inconsistencies as we performed some trials of the experiment individually, and not in groups of two or more as planned.</br></br>Justinmind was a great choice in software to develop our medium-fidelity prototype in. It allowed for an efficient way of recreating our low-fidelity prototype (see Blog Update #3), and resulted in a product that we could reliably test in our experiment. Prior to the creation of our medium-fidelity prototype, the cognitive walkthroughs performed on our paper prototype proved to be valuable as it allowed us to further develop our concept as described above.	</p>
					        <h3>5. Conclusions </h3>
					        <p>No statistically significant results were produced by this study. There is some evidence to suggest that users perform tasks faster when presented with an icon in replacement of a verbal/text label. There is also some evidence to suggest that individual percent matches lead to a greater subjective user confidence when given the choice between individual, combined, and verbal presentations. Conversely, participants disliked the verbal percent matches the most, claiming that the labels (ex. “worth a shot”) were too ambiguous. The common theme here is that participants want to know as much information as possible about the available routes in order to conduct their own analyses, rather than trusting our summaries and encodings. Further testing is necessary to determine which of the six conditions performs the best. For now, the evidence points to individual percentages with icons for the best matching filter.</p>
					        <h3>6. Recommendation </h3>
					        <p>Although we failed to reject our null hypotheses at the ⍺=0.05 level, we did see an increase in the mean confidence for our percent-match menu encoding. Given more time and resources, it would be valuable to run more of these studies with a larger subject pool in hopes of achieving statistical significance (in support of any hypothesis). It would interesting to further explore how our current set of conditions perform under different circumstances. Having one user perform multiple tasks under the same condition would help us to improve the depth of our results. It would also be interesting to experiment using a different set of filters; perhaps more rudimentary filters (fast versus slow, or scenic vs urban) would perform better than the three combinations presented in our current prototype.</br></br>Going forward, we should reduce the amount of ambiguity in our prototype, specifically with the route filter system, and how the icons are used to present resulting data. What seem to work well with our prototype that we should include more of is concrete data presentation, and system feedback. Users who were initially confused with the route filters, liked it when moving them resulted in change of information which helped informed them about our system.</br></br>One limitation in our current system is that users can’t dig deeper into a search result aside from what is presented on the results page. To properly assess confidence in a route, it would be intuitive for users to want to learn more about how that route operated to make a proper judgement. The main improvement here would be to make the prototype slightly more vertical to allow more room for exploration. Another way that we could generate better data would be if the back-end had a little more functionality to allow percentages to be calculated dynamically, based on a user’s inputted filter preferences. Implementing this feature would result in more realistic search results to be displayed, rather than the static set which works better for some tasks and not so well for others.</p>
					        <h3>Acknowledgements </h3>
					        <p>I would like to acknowledge Joanna McGrenere and Francesco Vitale for providing us with a framework under which to conduct this project and for their excellent feedback as to the strengths and weaknesses of our designs.</p>

                            <p>Course: UBC CPSC 444 </br>Subject: Advanced Methods for Human-Computer Interaction </br>Project Type: UX Design, User Testing, UX Research </br>Date: January-April 2017 </p>                   
                         
                            <button type="button" class="btn btn-default close-btn" data-dismiss="modal"><i class="fa fa-times"></i> Close</button>
                        </div>
                    </div>
                </div>
            </div>
	        </div>
	    </div>

	    <hr class="divider" id="contact">
		<div class="row no-padding">
			<div class="col-md-6 col-sm-6 col-xs-6 no-padding" >	
                <h3 class="contact-padding-l">Skills</h3>
				<p class="contact-padding-l">
				UX & UI design<br>
				Logo design<br>
				Motion design for UI<br>
				Computer Graphics<br>
				Web design & development<br>			
				Mobile & responsive design<br>
			    </p>
				<h1 class="heading" style="padding-left:40%; padding-top:0;"></h1>
			</div>
			<div class="col-md-6 col-sm-6 col-xs-6 no-padding container">
				<h3 class="contact-padding-r">Let's get in touch!</h3><p class="contact-padding-r">
					<a class="hover hvr-grow" href="mailto:buyunwang@hotmail.com" style="color:black;"><i class="fa fa-envelope-o" aria-hidden="true"></i> buyunwang@hotmail.com</a> </br>
					<a class="hover hvr-grow" href="https://www.linkedin.com/in/buyun-wang-8054167a/" style="color:black;"><i class="fa fa-linkedin" aria-hidden="true"></i> /buyunwang</a> </br>
					<a class="hover hvr-grow" href="https://www.behance.net/brianwang" style="color:black;"><i class="fa fa-behance" aria-hidden="true"></i> /brianwang</a> </br>
					<a class="hover hvr-grow" href="https://www.instagram.com/buyunwang/" style="color:black;"><i class="fa fa-instagram" aria-hidden="true"></i> /buyunwang</a> </br>
                    <a class="hover hvr-grow" href="https://github.com/buyunwang/" style="color:black;"><i class="fa fa-github" aria-hidden="true"></i> /buyunwang</a> </br>
				</p>
			</div>
		</div>

		<hr class="divider">

        <!-- FOOTER -->
        <footer>
          <p class="pull-right page-scroll"><a href="#page-top" style="color:black;" class="hover hvr-icon-float">Back to top</a></p>
          <p class="copyright">© <script> document.write(new Date().getFullYear()) </script> Brian Wang</p>
        </footer>


	</div>
	
 
</body>
</html>